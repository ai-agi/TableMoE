<div align="center">
<h1>TableMoE</h1>
<h3>Mixture-of-Connector-Experts for Multimodal Table Understanding</h3>

<sup></sup> Zhejiang University,

*If you have any question, feel free to contact [ðŸ“§](mailto:junwen.agi@gmail.com).*

</div>

**TableMoE** is a multimodal large language model empowered by a Mixture-of-Connector-Experts architecture, exhibiting strong conversational capabilities and trained on extensive tabular corpora to support advanced tasks such as table understanding, editing, replotting and beyond.

## News
- 2025.5.5: ðŸŽ‰ðŸŽ‰ðŸŽ‰ TableMoE extends the LLaVA-NeXT framework (https://github.com/LLaVA-VL/LLaVA-NeXT) by implementing new functionalities that enable encoding multiple images within multi-round conversations in a single sample per batchâ€”capabilities not supported in the original implementation!
- 2025.5.5: We release TableMoE!
