<div align="center">
  
<h1 style="display: flex; align-items: center; gap: 10px; margin: 0;">
  <img src="assets/wukong.png" alt="logo" height="44" />
  <span style="font-size: 2.6em; font-weight: 800;">TableMoE</span>
</h1>


<h3>Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding</h3>
<sup></sup>  College of Computer Science and Technology 
  
<sup></sup> Zhejiang University

*If you have any question, feel free to contact [📧](mailto:junwen.agi@gmail.com).*
</div>

**TableMoE** is a multimodal large language model empowered by a <strong>Mixture-of-Connector-Experts</strong> architecture, exhibiting strong conversational capabilities and trained on extensive tabular corpora to support advanced tasks such as table editing, table highlighting, table replotting, table transformation and beyond.

## News
- **`2025/06/02`**: 🌍 We release **WMMFinanceMath**, see [**here**](https://github.com/ai-agi/WMMFinanceMath ), a visually grounded version of FinanceMath with markdown tables rendered into WildStruct-style images featuring real-world noise like blur, skew, and watermarks. 🎉
- **`2025/05/05`**: 🎉🎉🎉 TableMoE extends the LLaVA-NeXT framework (https://github.com/LLaVA-VL/LLaVA-NeXT) by implementing new functionalities that enable encoding multiple images within multi-round conversations in a single sample per batch—capabilities not supported in the original implementation!
- **`2025/05/01`**: 🚀 We release TableMoE!
